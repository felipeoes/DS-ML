sempre dividir 70, 30 e setar uma seed (para poder replicar dps)

começar com AdaBoost
sempre gerar valor de KS

desenvolver modelo com 3 safras : agosto, setembro, outubro.
Aplicar em tds safras e dps gerar KS por safra, dps ver outoftime


depois que fazer o modelo com parâmetros aleatórios, utilizar o from sklearn.model_selection import RandomizedSearchCV
para otimização de hiperparâmetros

https://www.datacamp.com/community/tutorials/adaboost-classifier-python


***Shap não oferece suporte ao AdaBoost -> Exception: Model type not yet supported by TreeExplainer:
 <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>

-----Pros-----
AdaBoost is easy to implement. 
It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. 
You can use many base classifiers with AdaBoost. 
AdaBoost is not prone to overfitting. This can be found out via experiment results, 
but there is no concrete reason available.

-----Cons-----
AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. 
AdaBoost is slower compared to XGBoost.


Depois dar predict na safra de dezembro pra testar KS (LightGBM)